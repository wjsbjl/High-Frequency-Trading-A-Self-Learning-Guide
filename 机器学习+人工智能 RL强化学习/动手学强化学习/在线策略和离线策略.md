- 被比喻为洗脸盆洗手和水龙头冲手
- 区别在于$Q^\prime$到$a^\prime$的过程和$Q$到$a$的过程是否一致，不一致则是online
- Online Learning vs. Offline Learning: 提前收集好的数据（Dynamic Programming，DQN，Q-learning）还是环境交互收集数据（SARSA）
	- Online learning in reinforcement learning involves continuous interaction with the environment, where the agent makes decisions (actions), observes the outcomes (new states and rewards), and uses this information to update its policy or value function.
- Sarsa的$a$和$a^\prime$均由当前策略$\varepsilon-Greedy(Q)$采样，Q-learning的$a^\prime$由当前测量max(Q)采样
在线策略算法与离线策略算法
我们称采样数据的策略为行为策略（behavior policy），称用这些数据来更新的策略为目标策略（target policy）。在线策略（onpolicy）算法表示行为策略和目标策略是同一个策略；而离线策略（off-policy）算法表示行为策略和目标策略不是同一个策略。Sarsa 是典型的在线策略算法, 而 Q-learning 是典型的离线策略算法。判断二者类别的一个重要手段是看计算时序差分的价值目标的数据是否来自当前的策略，如图 5-1 所示。具体而言:
- 对于 Sarsa, 它的更新公式必须使用来自当前策略采样得到的五元组 $\left(s, a, r, s^{\prime}, a^{\prime}\right)$, 因此它是在线策略学习方法;
- 对于 Q-learning, 它的更新公式使用的是四元组 $\left(s, a, r, s^{\prime}\right)$ 来更新当前状态动作对的价值 $Q(s, a)$, 数据中的 $s$ 和 $a$ 是给定的条件, $r$ 和 $s^{\prime}$ 皆由环境采样得到, 该四元组并不需要一定是当前策略采样得到的数据, 也可以来自行为策略, 因此它是离线策略算法。
在本书之后的讲解中, 我们会注明各个算法分别属于这两类中的哪一类。如前文所述, 离线策略算法能够重复使用过往训练样本, 往往具有更小的样本复杂度, 也因此更受欢迎。