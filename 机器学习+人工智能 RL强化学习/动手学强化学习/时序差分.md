时序差分（temporal difference, TD）
- 用[[蒙特卡洛方法更新价值函数]]中，提到了[[增量式更新期望的方法（均值）]]，即$$V(s_t) \leftarrow V(s_{t})+\frac{1}{N(s)}(G_t-V(s_{t}))$$这里我们做一个变式$$
V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left[r_t+\gamma V\left(s_{t+1}\right)-V\left(s_t\right)\right]
$$其中 $R_t+\gamma V\left(s_{t+1}\right)-V\left(s_t\right)$ 通常被称为时序差分（temporal difference, TD）误差（error），时序差分算法将其与步长的乘积作为状态价值的更新量。
## 附录
可以用 $r_t+\gamma V\left(s_{t+1}\right)$ 来代替 $G_t$ 的原因是:
$$
\begin{aligned}
V_\pi(s) & =\mathbb{E}_\pi\left[G_t \mid S_t=s\right] \\
& =\mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k} \mid S_t=s\right] \\
& =\mathbb{E}_\pi\left[R_t+\gamma \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t=s\right] \\
& =\mathbb{E}_\pi\left[R_t+\gamma V_\pi\left(S_{t+1}\right) \mid S_t=s\right]
\end{aligned}
$$因此蒙特卡洛方法将上式第一行作为更新的目标, 而时序差分算法将上式最后一行作为更新的目标。于是, 在用策略和环境交互时, 每采样一步, 我们就可以用时序差分算法来更新状态价值估计。时序差分算法用到了 $V\left(s_{t+1}\right)$ 的估计值, 可以证明它最终收玫到策略 $\pi$ 的价值函数, 我们在此不对此进行展开说明。