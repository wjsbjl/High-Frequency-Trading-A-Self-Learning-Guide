- Markov decision process, MDP
- 由$\langle\mathcal{S}, \mathcal{A}, P, r, \gamma\rangle$ 构成, 其中:
	- $\mathcal{S}$ 是状态的集合;
	- $\mathcal{A}$ 是动作的集合;
	- $\gamma$ 是折扣因子;
	- $r(s, a)$ 是奖励函数, 此时奖励可以同时取决于状态 $s$ 和动作 $a$, 在奖励函数只取决于状态 $s$ 时, 则退化为 $r(s)$;
	- $P\left(s^{\prime} \mid s, a\right)$ 是状态转移函数, 表示在状态 $s$ 执行动作 $a$ 之后到达状态 $s^{\prime}$ 的概率。
- 策略$\pi(a|s)=P(A_t=a|S_t=s)$
## 基础概念
[[奖励，价值函数，贝尔曼期望方程]]