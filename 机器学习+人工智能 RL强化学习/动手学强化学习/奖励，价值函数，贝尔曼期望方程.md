- 在一个马尔可夫奖励过程中, 从第 $t$ 时刻状态 $S_t$ 开始, 直到终止状态时, 所有奖励的衰减之和称为回报 $G_t$ (Return), 公式如下:$$
G_t=R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+\cdots=\sum_{k=0}^{\infty} \gamma^k R_{t+k}
$$
- 状态价值函数
	- 我们用 $V^\pi(s)$ 表示在 MDP 中基于策略 $\pi$ 的状态价值函数（state-value function），定义为从状态 $s$ 出发遵循策略 $\pi$ 能获得的期望回报，数学表达为:$$
V^\pi(s)=\mathbb{E}_\pi\left[G_t \mid S_t=s\right]
$$
- 动作价值函数
	- 不同于 MRP，在 MDP 中，由于动作的存在，我们额外定义一个动作价值函数（action-value function）。我们用 $Q^\pi(s, a)$ 表示在 MDP 遵循策略 $\pi$ 时, 对当前状态 $s$ 执行动作 $a$ 得到的期望回报:$$
Q^\pi(s, a)=\mathbb{E}_\pi\left[G_t \mid S_t=s, A_t=a\right]
$$
	- 状态价值函数和动作价值函数之间的关系：在使用策略 $\pi$ 中，状态 $s$ 的价值等于在该状态下基于策略 $\pi$ 采取所有动作的概率与相应的价值相乘再求和的结果:$$
V^\pi(s)=\sum_{a \in A} \pi(a \mid s) Q^\pi(s, a)
$$
	- 使用策略 $\pi$ 时, 状态 $s$ 下采取动作 $a$ 的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积：
$$
Q^\pi(s, a)=r(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) V^\pi\left(s^{\prime}\right)
$$
- 贝尔曼期望方程
	- 在贝尔曼方程中加上“期望”二字是为了与接下来的贝尔曼最优方程进行区分。我们通过简单推导就可以分别得到两个价值函数的贝尔曼期望方程（Bellman Expectation Equation）：$$
\begin{aligned}
V^\pi(s) & =\mathbb{E}_\pi\left[R_t+\gamma V^\pi\left(S_{t+1}\right) \mid S_t=s\right] \\
& =\sum_{a \in A} \pi(a \mid s)\left(r(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V^\pi\left(s^{\prime}\right)\right) \\
Q^\pi(s, a) & =\mathbb{E}_\pi\left[R_t+\gamma Q^\pi\left(S_{t+1}, A_{t+1}\right) \mid S_t=s, A_t=a\right] \\
& =r(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) Q^\pi\left(s^{\prime}, a^{\prime}\right)
\end{aligned}
$$
