- 蒙特卡洛方法（Monte-Carlo methods）是一种统计模拟方法，重复随机抽样，用概率统计方法归纳出数值统计结果
==用未来期望奖励的均值作为值函数估计==
我们现在介绍如何用蒙特卡洛方法来估计一个策略在一个马尔可夫决策过程中的状态价值函数。回忆一下，一个状态的价值是它的期望回报, 那么一个很直观的想法就是用策略在 MDP 上采样很多条序列, 计算从这个状态出发的回报再求其期望就可以了, 公式如下：
$$
V^\pi(s)=\mathbb{E}_\pi\left[G_t \mid S_t=s\right] \approx \frac{1}{N} \sum_{i=1}^N G_t^{(i)}
$$
在一条序列中, 可能没有出现过这个状态, 可能只出现过一次这个状态, 也可能出现过很多次这个状态。我们介绍的蒙特卡洛价值估计方法会在该状态每一次出现时计算它的回报。还有一种选择是一条序列只计算一次回报, 也就是这条序列第一次出现该状态时计算后面的累积奖励, 而后面再次出现该状态时, 该状态就被忽略了。假设我们现在用策略 $\pi$ 从状态 $s$ 开始采样序列, 据此来计算状态价值。我们为每一个状态维护一个计数器和总回报, 计算状态价值的具体过程如下所示。
(1) 使用策略 $\pi$ 采样若干条序列:
$$
s_0^{(i)} \xrightarrow{a_0^{(i)}} r_0^{(i)}, s_1^{(i)} \xrightarrow{a_1^{(i)}} r_1^{(i)}, s_2^{(i)} \xrightarrow{a_2^{(i)}} \cdots \xrightarrow{a_{T-1}^{(i)}} r_{T-1}^{(i)}, s_T^{(i)}
$$
(2) 对每一条序列中的每一时间步 $t$ 的状态 $s$ 进行以下操作:
- 更新状态 $s$ 的计数器 $N(s) \leftarrow N(s)+1$;
- 更新状态 $s$ 的总回报 $M(s) \leftarrow M(s)+G_t$;
(3) 每一个状态的价值被估计为回报的平均值 $V(s)=M(s) / N(s)$ 。

根据大数定律, 当 $N(s) \rightarrow \infty$, 有 $V(s) \rightarrow V^\pi(s)$ 。计算回报的期望时, 除了可以把所有的回报加起来除以次数, 还有一种增量更新的方法。对于每个状态 $s$ 和对应回报 $G$, 进行如下计算:
- $N(s) \leftarrow N(s)+1$
- $V(s) \leftarrow V(s)+\frac{1}{N(s)}(G-V(S))$

这种[[增量式更新期望的方法（均值）]]已经在第 2 章中展示过。
==V是G的平均，新G以平均差额的形式更新进来==
$$\begin{aligned}
V_t&=\frac{1}{t}\sum_{i=1}^tG_i\\
&=\frac{1}{t}(G_t+\sum_{i=1}^{t-1}G_i)\\
&=\frac{1}{t}(G_t+(t-1)V_{t-1})\\
&=V_{t-1}+\frac{1}{t}(G_t-V_{t-1})\\
\end{aligned}$$