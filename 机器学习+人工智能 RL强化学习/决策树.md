- 分类和回归树（classification and regression tree，CART）
	- 用误差平方和作为回归问题下选取最优特征的标准
- bagging
	- 随机抽取样本训练，然后合一起
- 随机森林
	- 在bagging基础上，每次分裂钱，从全部$M$个特征中采样$m$个特征，在这里面找最优，一般$m=\sqrt{M}$

- 条件熵：给定$Y$分布后$X$的熵
	- 在第6章逻辑斯谛回归中, 我们已经介绍了信息量与信息熵的概念, 这里再直接从随机变量的角度回顾一下。设离散随机变量 $X$ 和 $Y$ 的取值范围都是 $1, \ldots, n$, 那么 $X$ 的熵为:$$
H(X)=-\sum_{i=1}^n P(X=i) \log P(X=i)
$$如果将随机变量 $X$ 取值为 $i$ 看作事件 $X_i$ 的话, 那么其分布就是 $p\left(X_i\right)=P(X=i)$, 这样上式和逻辑斯谛回归一章中给出的公式是相同的。因此, 下面直接将 $X$ 取值为 $i$ 简写为 $X_i$, 将 $Y$ 取值为 $j$ 简写为 $Y_j$ 。随机变量 $X$ 与 $Y$的交叉摘为:$$
H(X, Y)=-\sum_{i=1}^n P\left(X_i\right) \log P\left(Y_i\right)
$$我们通常用概率分布的摘的变化来作为信息的度量。当我们为一个分布引入额外信息时, 其不确定度会减小,所以摘会降低。为此, 我们引入条件摘的概念。在给定 $Y=j$ 的条件下, 随机变量 $X$ 的熵为：$$
H\left(X \mid Y_j\right)=-\sum_{i=1}^n P\left(X_i \mid Y_j\right) \log P\left(X_i \mid Y_j\right)
$$如果随机变量 $X$ 与 $Y$ 独立, 那么对于任意 $i$ 有条件概率 $P\left(X_i \mid Y_j\right)=P\left(X_i\right)$, 对于任意 $j$ 有条件熵 $H\left(X \mid Y_j\right)=H(X)$, 说明 $Y$ 的取值对 $X$ 的分布没有影响。如果随机变量 $X$ 与 $Y$ 始终相同, 那么 $P\left(X_i \mid Y_j\right)=\mathbb{I}(i=j)$, 条件熵 $H\left(X \mid Y_j\right)=0$, 说明 $Y$ 的取值确定后 $X$ 的取值也唯一确定, 其随机性消失, 分布的熵变为 0 。进一步, 如果给出的条件是 $Y$ 的分布, 那么条件熵 $H(X \mid Y)$ 为$$
H(X \mid Y)=\mathbb{E}_Y\left[H\left(X \mid Y_j\right)\right]=\sum_{j=1}^n P\left(Y_j\right) H\left(X \mid Y_j\right)=-\sum_{i=1}^n \sum_{j=1}^n P\left(X_i, Y_j\right) \log P\left(X_i \mid Y_j\right)
$$
- 信息增益$$I(X|Y)=H(X)-H(X|Y)$$
- 信息增益率$$I_R(X,Y)=\frac{I(X,Y)}{H_Y(X)}$$
	- 其中$$H_Y(X)=-\sum_{y\in{\mathcal{Y}}}\frac{\#X_{Y=y}}{\#X}\log\frac{\#X_{Y=y}}{\#{X}}$$$\#X$表示样本总数量，$\#X_{Y=y}$表示样本中特征Y=y的数量
	- $H_Y(X)$和$H(X|Y)$的区别为，$H(X|Y)$是联合分布，样本只考虑对应特征内的。$H_Y(X)$是总的分布，考虑所有样本